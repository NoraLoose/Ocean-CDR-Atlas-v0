{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1cf506-e451-47ce-8d7a-bc675d39a1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pathlib\n",
    "import xarray as xr\n",
    "import dask\n",
    "import loky\n",
    "import ndpyramid\n",
    "import os\n",
    "import fsspec\n",
    "import traceback\n",
    "from rich.console import Console\n",
    "import tqdm\n",
    "\n",
    "parent_dir = pathlib.Path.cwd().parent\n",
    "sys.path.append(str(parent_dir))\n",
    "sys.path.append(str(parent_dir.parent))\n",
    "import functools\n",
    "from dor_config import DORConfig\n",
    "\n",
    "@property\n",
    "def compressed_data_dir(self) -> pathlib.Path:\n",
    "    \"\"\"Get the compressed data directory and ensure it exists.\"\"\"\n",
    "    out_dir = (\n",
    "        pathlib.Path(self.parent_data_dir)\n",
    "        / \"research-grade-compressed\"\n",
    "       \n",
    "    )\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return out_dir\n",
    "\n",
    "DORConfig.compressed_data_dir = compressed_data_dir\n",
    "\n",
    "\n",
    "from dor_cli import setup_directories\n",
    "\n",
    "\n",
    "from vis_pyramid import create_template_store2, get_nc_glob_pattern, load_ssh_data, reduction, integrate_column_mol, reshape_into_month_year, setup_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0404d1-cf4d-4bda-b75f-5dd7420632d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()\n",
    "\n",
    "dirs = setup_directories()\n",
    "memory = setup_memory(dirs[\"joblib_cache_dir\"])\n",
    "memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb54ee-bee0-4972-a358-ad9f4dc3f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DORConfig(parent_data_dir='/global/cfs/projectdirs/m4746/Datasets/Ocean-CDR-Atlas-v0/OAE-Efficiency-Map/', \n",
    "                   store_1_path='/pscratch/sd/a/abanihi/oae/store1b.zarr', \n",
    "                   store_2_path='s3://carbonplan-oae-efficiency/v3/store2.zarr/',\n",
    "                  )\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c85306-6052-4d15-a281-10aeb0d10c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.compressed_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5109debb-2150-41a2-b2c0-da2260017dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create_template_store2(output_store=config.store_2_path, variables=[\"DIC\", \"ALK\", \"FG\", \"PH\", \"pCO2SURF\"], levels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd5623-a627-4c9d-8468-e533bce2c85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduction(ds, ssh):\n",
    "    with xr.set_options(keep_attrs=True):\n",
    "        # [\"DIC\", \"ALK\", \"FG\", \"PH\", \"pCO2SURF\"]\n",
    "        alk_delta_surf = ds.ALK.isel(z_t=0) - ds.ALK_ALT_CO2.isel(z_t=0)\n",
    "        PH_delta_surf = ds.PH - ds.PH_ALT_CO2\n",
    "        pco2_delta_surf = ds.pCO2SURF - ds.pCO2SURF_ALT_CO2\n",
    "        fg_co2_delta_surf = ds.FG_CO2 - ds.FG_ALT_CO2\n",
    "\n",
    "        dic_column_integrated = integrate_column_mol(ds.DIC, ds[\"dz\"], ssh)\n",
    "        dic_delta_column_integrated = dic_column_integrated - integrate_column_mol(\n",
    "            ds.DIC_ALT_CO2, ds[\"dz\"], ssh\n",
    "        ) \n",
    "        dso = dict(\n",
    "                ALK_SURF=ds.ALK.isel(z_t=0),\n",
    "                ALK_DELTA_SURF = alk_delta_surf,\n",
    "                FG_CO2_SURF=ds.FG_CO2,\n",
    "                FG_CO2_DELTA_SURF=fg_co2_delta_surf,\n",
    "                DIC_COLUMN_INTEGRATED=dic_column_integrated,\n",
    "                DIC_DELTA_COLUMN_INTEGRATED=dic_delta_column_integrated,\n",
    "                PH_SURF=ds.PH,\n",
    "                PH_DELTA_SURF=PH_delta_surf,\n",
    "                pCO2_DELTA_SURF=pco2_delta_surf,\n",
    "                pCO2_SURF=ds.pCO2SURF,\n",
    "            \n",
    "        )\n",
    "        dset = xr.Dataset(dso)\n",
    "        coords_to_drop = set(dset.coords).difference(set(['polygon_id', 'injection_date', 'elapsed_time', 'ULONG', 'ULAT']))\n",
    "        return dset.drop_vars(coords_to_drop)\n",
    "\n",
    "def concatenate_into_bands(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Concatenate the dataset into bands.\"\"\"\n",
    "    bands_ds = xr.Dataset(coords=ds.coords)\n",
    "\n",
    "    bands_ds[\"ALK\"] = xr.concat(\n",
    "        [ds[\"ALK_DELTA_SURF\"], ds[\"ALK_SURF\"]],\n",
    "        dim=xr.DataArray(name=\"band\", data=[\"delta\", \"experimental\"], dims=\"band\"),\n",
    "    )\n",
    "    bands_ds[\"DIC\"] = xr.concat(\n",
    "        [ds[\"DIC_DELTA_COLUMN_INTEGRATED\"], ds[\"DIC_COLUMN_INTEGRATED\"]],\n",
    "        dim=xr.DataArray(name=\"band\", data=[\"delta\", \"experimental\"], dims=\"band\"),\n",
    "    )\n",
    "    bands_ds[\"PH\"] = xr.concat(\n",
    "        [ds[\"PH_DELTA_SURF\"], ds[\"PH_SURF\"]],\n",
    "        dim=xr.DataArray(name=\"band\", data=[\"delta\", \"experimental\"], dims=\"band\"),\n",
    "    )\n",
    "    bands_ds[\"FG\"] = xr.concat(\n",
    "        [ds[\"FG_CO2_DELTA_SURF\"], ds[\"FG_CO2_SURF\"]],\n",
    "        dim=xr.DataArray(name=\"band\", data=[\"delta\", \"experimental\"], dims=\"band\"),\n",
    "    )\n",
    "    bands_ds[\"pCO2SURF\"] = xr.concat(\n",
    "        [ds[\"pCO2_DELTA_SURF\"], ds[\"pCO2_SURF\"]],\n",
    "        dim=xr.DataArray(name=\"band\", data=[\"delta\", \"experimental\"], dims=\"band\"),\n",
    "    )\n",
    "\n",
    "    return bands_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbbedb8-b40c-4394-8a08-19aa90de7262",
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def process_and_create_pyramid(\n",
    "    polygon_id: str,\n",
    "    injection_month: str,\n",
    "    data_dir: str,\n",
    "    store_path: str,\n",
    "    weights_store: str,\n",
    "    levels: int = 2,\n",
    ") -> None:\n",
    "    \"\"\"Process data and create visualization pyramid.\"\"\"\n",
    "    try:\n",
    "        path = get_nc_glob_pattern(data_dir, polygon_id, injection_month)\n",
    "        console.print(f\"Loading data from {path}\", style=\"blue\")\n",
    "\n",
    "        with dask.config.set(\n",
    "            pool=loky.ProcessPoolExecutor(max_workers=os.cpu_count() // 2, timeout=120)\n",
    "        ):\n",
    "            ds = xr.open_mfdataset(\n",
    "                path,\n",
    "                coords=\"minimal\",\n",
    "                combine=\"by_coords\",\n",
    "                data_vars=\"minimal\",\n",
    "                compat=\"override\",\n",
    "                decode_times=True,\n",
    "                parallel=True,\n",
    "                decode_timedelta=True,\n",
    "            )\n",
    "            ds = dask.optimize(ds)[0]\n",
    "\n",
    "            console.print(\"Processing dataset through reduction pipeline\", style=\"blue\")\n",
    "            ssh = load_ssh_data(injection_month, ssh_path=f\"{config.compressed_data_dir}/control/g.e22.GOMIPECOIAF_JRA-1p4-2018.TL319_g17.SMYLE.005.pop.h.SSH.030601-036812.nc\")\n",
    "\n",
    "            \n",
    "\n",
    "            bands_ds = (\n",
    "                ds.pipe(reduction, ssh)\n",
    "                .pipe(concatenate_into_bands)\n",
    "                .pipe(reshape_into_month_year)\n",
    "            )\n",
    "\n",
    "            console.print(\"Building visualization pyramid\", style=\"blue\")\n",
    "            other_chunks = dict(\n",
    "                month=1, year=-1, band=1, polygon_id=1, injection_date=1, x=128, y=128\n",
    "            )\n",
    "\n",
    "            if fsspec.get_mapper(weights_store).fs.exists(weights_store):\n",
    "                console.print(\n",
    "                    f\"Using weights from {weights_store} for regridding\", style=\"blue\"\n",
    "                )\n",
    "                weights = xr.open_datatree(weights_store, engine=\"zarr\", chunks={})\n",
    "\n",
    "            else:\n",
    "                console.print(\n",
    "                    \"No weights store provided or does not exist. \"\n",
    "                    \"Weights will be generated on-the-fly.\",\n",
    "                    style=\"yellow\",\n",
    "                )\n",
    "                weights = ndpyramid.regrid.generate_weights_pyramid(bands_ds, levels=2)\n",
    "                weights.to_zarr(\n",
    "                    weights_store, consolidated=True, zarr_format=2, mode=\"w\"\n",
    "                )\n",
    "\n",
    "            pyramid = ndpyramid.pyramid_regrid(\n",
    "                bands_ds,\n",
    "                levels=levels,\n",
    "                projection=\"web-mercator\",\n",
    "                parallel_weights=False,\n",
    "                other_chunks=other_chunks,\n",
    "                weights_pyramid=weights,\n",
    "            )\n",
    "\n",
    "            pyramid = dask.optimize(pyramid)[0]\n",
    "\n",
    "            console.print(f\"Saving pyramid to {store_path}\", style=\"blue\")\n",
    "            pyramid.to_zarr(store_path, region=\"auto\", mode=\"r+\")\n",
    "\n",
    "            return pyramid\n",
    "\n",
    "    except Exception as exc:\n",
    "        console.print(\n",
    "            f\"[bold red]Error processing polygon_id={polygon_id}, \"\n",
    "            f\"injection_month={injection_month}: {traceback.format_exc()}[/bold red]\"\n",
    "        )\n",
    "        raise exc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a552b412-636c-45bb-9e26-458bac5585c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "polygon_ids = range(0, 690)\n",
    "padded_polygon_ids = [f\"{polygon_id:03d}\" for polygon_id in polygon_ids]\n",
    "padded_injection_months = ['01', '04', '07', '10']\n",
    "tasks = []\n",
    "for polygon_id in padded_polygon_ids:\n",
    "    for injection_month in padded_injection_months:\n",
    "        tasks.append((polygon_id, injection_month))\n",
    "tasks[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c04fdf2-97e9-4d18-98c0-6a48d808feaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for polygon_id, injection_month in tqdm.tqdm(tasks):\n",
    "    try:\n",
    "        process_and_create_pyramid(polygon_id=polygon_id, injection_month=injection_month, \n",
    "                               data_dir=f'{config.compressed_data_dir}/experiments', \n",
    "                               store_path=config.store_2_path, \n",
    "                               weights_store=f\"{os.environ['SCRATCH']}/oae/weights.zarr\")\n",
    "        #console.print(f\"Finished processing polygon_id={polygon_id}, injection_month={injection_month}\",style=\"green\")\n",
    "    except Exception:\n",
    "            console.print(\n",
    "                f\"[bold red]Error processing {polygon_id}/{injection_month}: \"\n",
    "                f\"{traceback.format_exc()}[/bold red]\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dor",
   "language": "python",
   "name": "dor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
